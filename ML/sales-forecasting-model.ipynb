{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2022-05-05T12:51:04.327889Z","iopub.execute_input":"2022-05-05T12:51:04.328195Z","iopub.status.idle":"2022-05-05T12:51:04.347461Z","shell.execute_reply.started":"2022-05-05T12:51:04.328155Z","shell.execute_reply":"2022-05-05T12:51:04.346542Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"**Intro**\n\nThe aim of this project is to demonstrate a simple model for forecasting upcoming sales. There are a huge number of ways to deal with this, including framing this as either a classification or regression problem, and also a range of models one can use. In this case, we will read in our data, and compile a problem statement, workflow and model, as if in a real world business use case. As always, we first import the relevant libraries, read in our data and do a basic overview analysis.\n\nThis Kaggle dataset features 2 restaurants and there are 4 files in total. In a normal business setting we would be using SQL, in a relational database, but for this notebook, I will carry out the same operations in python. For ease, I am going to only only forecast Restaurant 2.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, r2_score, mean_squared_log_error\nfrom xgboost import XGBRegressor","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:04.513917Z","iopub.execute_input":"2022-05-05T12:51:04.515222Z","iopub.status.idle":"2022-05-05T12:51:04.520271Z","shell.execute_reply.started":"2022-05-05T12:51:04.515172Z","shell.execute_reply":"2022-05-05T12:51:04.519431Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"orders = pd.read_csv(\"../input/19560-indian-takeaway-orders/restaurant-2-orders.csv\")\nproducts = pd.read_csv(\"../input/19560-indian-takeaway-orders/restaurant-2-products-price.csv\")\norders","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:04.621462Z","iopub.execute_input":"2022-05-05T12:51:04.621998Z","iopub.status.idle":"2022-05-05T12:51:04.776474Z","shell.execute_reply.started":"2022-05-05T12:51:04.621954Z","shell.execute_reply":"2022-05-05T12:51:04.775704Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"products.head(10)\n#from this we can see that we didn't actually need the products data. It is already joined into the orders database\ndel products","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:04.795491Z","iopub.execute_input":"2022-05-05T12:51:04.795779Z","iopub.status.idle":"2022-05-05T12:51:04.800191Z","shell.execute_reply.started":"2022-05-05T12:51:04.795747Z","shell.execute_reply":"2022-05-05T12:51:04.799179Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"#drop nan values and convert to datetime\norders = orders.dropna()\norders['Order Date'] = pd.to_datetime(orders['Order Date'], dayfirst = True)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:04.952652Z","iopub.execute_input":"2022-05-05T12:51:04.953081Z","iopub.status.idle":"2022-05-05T12:51:06.885562Z","shell.execute_reply.started":"2022-05-05T12:51:04.953033Z","shell.execute_reply":"2022-05-05T12:51:06.884716Z"},"trusted":true},"execution_count":137,"outputs":[]},{"cell_type":"code","source":"#check how bad the time gaps are in the data\n#first we group orders by day, summing total products\ntime_grouped = orders[['Order Date', 'Quantity']].resample('D', on='Order Date').sum().reset_index()\ntime_grouped \n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:06.887267Z","iopub.execute_input":"2022-05-05T12:51:06.887581Z","iopub.status.idle":"2022-05-05T12:51:06.942451Z","shell.execute_reply.started":"2022-05-05T12:51:06.887539Z","shell.execute_reply":"2022-05-05T12:51:06.941731Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"code","source":"#now we make a list of xticks\ntimes = pd.date_range(start='2016-06-10',end='2019-08-03')\n#and plot\nplt.figure(figsize = (40,10))\nsns.scatterplot(x = time_grouped['Order Date'], y = time_grouped['Quantity'])\nplt.xticks(times)\nplt.plot","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:06.943648Z","iopub.execute_input":"2022-05-05T12:51:06.943932Z","iopub.status.idle":"2022-05-05T12:51:22.443405Z","shell.execute_reply.started":"2022-05-05T12:51:06.943902Z","shell.execute_reply":"2022-05-05T12:51:22.442556Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"#The plot shows generally few gaps for the latter portion, though there are 2 flats early on.\n#Lets zoom in on this part of the graph\n#and plot\nplt.figure(figsize = (20,20))\nsns.scatterplot(x = time_grouped['Order Date'].iloc[200:525], y = time_grouped['Quantity'].iloc[200:525])\nplt.plot\n#we can see that from 08/2016 is when orders become consistent, so these the values we will use for the model\norders = orders.loc[orders['Order Date'] >='2016-08-01' ]","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:22.445264Z","iopub.execute_input":"2022-05-05T12:51:22.445474Z","iopub.status.idle":"2022-05-05T12:51:22.867364Z","shell.execute_reply.started":"2022-05-05T12:51:22.445447Z","shell.execute_reply":"2022-05-05T12:51:22.866556Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"#Groupby items to see what's popular\ngrouped = orders.groupby('Item Name').sum().sort_values(by = 'Total products', ascending = 0)\ngrouped\n#we can see there are 337 unique products, with some much more popular than others","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:22.868566Z","iopub.execute_input":"2022-05-05T12:51:22.868895Z","iopub.status.idle":"2022-05-05T12:51:22.914222Z","shell.execute_reply.started":"2022-05-05T12:51:22.868862Z","shell.execute_reply":"2022-05-05T12:51:22.913172Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"#Investigating average order volume by periods\nprint(\"Daily:\\n\", orders.groupby([pd.Grouper(key='Order Date', freq='D')])['Quantity'].sum().mean())\nprint(\"Weekly:\\n\", orders.groupby([pd.Grouper(key='Order Date', freq='W-MON')])['Quantity'].sum().mean())\nprint(\"Monthly:\\n\", orders.groupby([pd.Grouper(key='Order Date', freq='M')])['Quantity'].sum().mean())","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:22.915776Z","iopub.execute_input":"2022-05-05T12:51:22.916028Z","iopub.status.idle":"2022-05-05T12:51:23.072454Z","shell.execute_reply.started":"2022-05-05T12:51:22.915997Z","shell.execute_reply":"2022-05-05T12:51:23.071596Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"**1. Define Problem Statement**\n\n\nIn this case, letâ€™s say we are the owner of the takeaway business. Why would forecasting be useful? In this case, it may be helpful to know how may orders we will receive in total, so that we can get adequate staffing in place, without overpaying for staff unnecessarily. It may also be worth knowing what the sales of major items are likely to be, to ensure these do not run out.\n\nSo to address this case, the best method is to model as a regression problem. In the real world the likely situation would be a periodically run model or models, predicting x time period(s) ahead, which can be retrained/updated as needed either manually, or through an automated process. For this notebook, we will show the first step, which is a trained model ready to deploy. The process could then be manually updated/optimised monthly.\n\n*** Based on the simple EDA above, the daily volume is too small and the monthly too large,  so we will create a model which predicts Total Sales Volume for the next week, as well as next week's sales volume for Bombay Aloo separately, as this is the most popular item which is not a side/condiment**\n","metadata":{}},{"cell_type":"markdown","source":"First we orgainise our table by date, drop unnecessary columns, and feature engineer, as well as creating labels","metadata":{}},{"cell_type":"code","source":"#create relevant Database df1 for total and df2 for bombay aloo\ndf = orders[['Order Date', 'Quantity']]\ndf2 = orders[orders['Item Name'] == 'Bombay Aloo']\ndf2 = df2[['Order Date', 'Quantity']]\ndf = df.groupby([pd.Grouper(key='Order Date', freq='W-MON')])['Quantity'].sum().reset_index().sort_values('Order Date')\ndf2 = df2.groupby([pd.Grouper(key='Order Date', freq='W-MON')])['Quantity'].sum().reset_index().sort_values('Order Date')\n#Add Seasonality features\ndf['Week'] = df['Order Date'].dt.isocalendar().week\ndf['Month'] = df['Order Date'].dt.month\ndf2['Week'] = df2['Order Date'].dt.isocalendar().week\ndf2['Month'] = df2['Order Date'].dt.month\n#Add past volume features\nfor i in range (1,15):\n    label = \"Quantity_\" + str(i)\n    df[label] = df['Quantity'].shift(i)\n    df2[label] = df2['Quantity'].shift(i)\n    label = \"Average_\" + str(i)\n    df[label] = df['Quantity'].rolling(i).mean()\n    df2[label] = df2['Quantity'].rolling(i).mean()\ndf = df.dropna()\ndf2 = df2.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:56:18.014985Z","iopub.execute_input":"2022-05-05T12:56:18.015414Z","iopub.status.idle":"2022-05-05T12:56:18.144228Z","shell.execute_reply.started":"2022-05-05T12:56:18.015382Z","shell.execute_reply":"2022-05-05T12:56:18.143585Z"},"trusted":true},"execution_count":154,"outputs":[]},{"cell_type":"code","source":"#one hot encode df using pandas get_dummies\nfor column in ['Week','Month']:\n    tempdf = pd.get_dummies(df[column], prefix=column)\n    df = pd.merge(\n        left=df,\n        right=tempdf,\n        left_index=True,\n        right_index=True,\n    )\n    df = df.drop(columns=column)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:56:21.008181Z","iopub.execute_input":"2022-05-05T12:56:21.008712Z","iopub.status.idle":"2022-05-05T12:56:21.026127Z","shell.execute_reply.started":"2022-05-05T12:56:21.008660Z","shell.execute_reply":"2022-05-05T12:56:21.025268Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"#one hot encode df2 using pandas get_dummies\nfor column in ['Week','Month']:\n    tempdf = pd.get_dummies(df2[column], prefix=column)\n    df2 = pd.merge(\n        left=df2,\n        right=tempdf,\n        left_index=True,\n        right_index=True,\n    )\n    df2 = df2.drop(columns=column)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:56:23.654888Z","iopub.execute_input":"2022-05-05T12:56:23.655210Z","iopub.status.idle":"2022-05-05T12:56:23.672037Z","shell.execute_reply.started":"2022-05-05T12:56:23.655173Z","shell.execute_reply":"2022-05-05T12:56:23.671127Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"code","source":"df2.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:56:30.972537Z","iopub.execute_input":"2022-05-05T12:56:30.973043Z","iopub.status.idle":"2022-05-05T12:56:31.000524Z","shell.execute_reply.started":"2022-05-05T12:56:30.972995Z","shell.execute_reply":"2022-05-05T12:56:30.999431Z"},"trusted":true},"execution_count":157,"outputs":[]},{"cell_type":"code","source":"#143 rows so we split the data up to  row 107 for train and test sets for df\ntrain = df[:107].drop('Order Date', axis = 1)\ntest = df[107:].drop('Order Date', axis = 1)\nxtrain = train.drop(['Quantity'], axis = 1)\nxtest = test.drop(['Quantity'], axis = 1)\nytrain = train['Quantity']\nytest =test['Quantity']","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:51:23.167867Z","iopub.status.idle":"2022-05-05T12:51:23.168442Z","shell.execute_reply.started":"2022-05-05T12:51:23.168202Z","shell.execute_reply":"2022-05-05T12:51:23.168227Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#143 rows so we split the data up to  row 107 for train and test sets for df2\ntrain2 = df2[:107].drop('Order Date', axis = 1)\ntest2 = df2[107:].drop('Order Date', axis = 1)\nxtrain2 = train2.drop(['Quantity'], axis = 1)\nxtest2 = test2.drop(['Quantity'], axis = 1)\nytrain2 = train2['Quantity']\nytest2 =test2['Quantity']","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:58:52.715820Z","iopub.execute_input":"2022-05-05T12:58:52.716153Z","iopub.status.idle":"2022-05-05T12:58:52.726273Z","shell.execute_reply.started":"2022-05-05T12:58:52.716108Z","shell.execute_reply":"2022-05-05T12:58:52.725650Z"},"trusted":true},"execution_count":160,"outputs":[]},{"cell_type":"markdown","source":"**2. Build Model**\n\nNow we build a model. There are various models we could use including the (S)ARIMA(X) models, and FBProphet, as well as a LSTM network in Keras. Because of the small dataset size and poor quality, and the sparsity of the data, plus the 2018 sales drop due to the global financial crisis, nothing is really going to be any good at making predictions. But in the abscence of more data length, and more explanatory variables, I have chosen XGBoost (it can handle sparcity without needing to convert to CSR and can handle multiple regressors).\n\nWe could run this model and save the results through a loop, to run it across multipel epochs for further into the future predictions. But as per our problem statement, we are going to focus on just the next week at a time (in a real life situation, as the busniess owner, we may run this at the end of each week, to get orders and staff prepared over the weekend for next week).","metadata":{}},{"cell_type":"code","source":"#Model for df\nmodel = XGBRegressor(n_estimators=500, learning_rate=0.01)\neval_set = [(xtrain, ytrain)]\nmodel.fit(xtrain, ytrain, eval_metric=\"rmsle\", eval_set=eval_set, early_stopping_rounds=20, verbose=False)\nypred = model.predict(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:59:19.430058Z","iopub.execute_input":"2022-05-05T12:59:19.430358Z","iopub.status.idle":"2022-05-05T12:59:21.962877Z","shell.execute_reply.started":"2022-05-05T12:59:19.430325Z","shell.execute_reply":"2022-05-05T12:59:21.962252Z"},"trusted":true},"execution_count":163,"outputs":[]},{"cell_type":"code","source":"#Model for df2\nmodel2 = XGBRegressor(n_estimators=500, learning_rate=0.01)\neval_set = [(xtrain2, ytrain2)]\nmodel2.fit(xtrain2, ytrain2, eval_metric=\"rmsle\", eval_set=eval_set, early_stopping_rounds=20, verbose=False)\nypred2 = model2.predict(xtest2)","metadata":{"execution":{"iopub.status.busy":"2022-05-05T12:59:23.664603Z","iopub.execute_input":"2022-05-05T12:59:23.666801Z","iopub.status.idle":"2022-05-05T12:59:26.418819Z","shell.execute_reply.started":"2022-05-05T12:59:23.666718Z","shell.execute_reply":"2022-05-05T12:59:26.418000Z"},"trusted":true},"execution_count":164,"outputs":[]},{"cell_type":"markdown","source":"**3. Evaluation**\n\nFinally, I evaluate both the models: both on graphs, and then using the relevant metrics. We optmised scoring for root mean square log error as it standardises for when a quantity is already high and thus the absolute percentage change is low","metadata":{}},{"cell_type":"code","source":"#First we add the results to our original dataframe, after first aligning the indexes\n\n#df\nypred = pd.Series(ypred)\neval_df = df[107:].reset_index(drop = True)\neval_df['ypred'] = ypred\neval_df = eval_df[['Order Date','Quantity', 'ypred']]\neval_df.head()\n\n#df2\nypred2 = pd.Series(ypred2)\neval_df2 = df2[107:].reset_index(drop = True)\neval_df2['ypred'] = round(ypred2)\neval_df2 = eval_df2[['Order Date','Quantity', 'ypred']]\neval_df2.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:01:58.421320Z","iopub.execute_input":"2022-05-05T13:01:58.421927Z","iopub.status.idle":"2022-05-05T13:01:58.442555Z","shell.execute_reply.started":"2022-05-05T13:01:58.421878Z","shell.execute_reply":"2022-05-05T13:01:58.441928Z"},"trusted":true},"execution_count":166,"outputs":[]},{"cell_type":"code","source":"#And Now we plot the results of the train vs test sets\n#df\nplt.figure(figsize = (20,8))\nplt.plot(eval_df['Order Date'], eval_df['Quantity'], label = \"Actual Quanitity\")\nplt.plot(eval_df['Order Date'], eval_df['ypred'], color = 'red', label = 'Predicted Quantity')\nplt.xlabel('Date')\nplt.ylabel('Quantity')\nplt.legend()\nplt.title('Total Sales')\n\n#df2\nplt.figure(figsize = (20,8))\nplt.plot(eval_df2['Order Date'], eval_df2['Quantity'], label = \"Actual Quanitity\")\nplt.plot(eval_df2['Order Date'], eval_df2['ypred'], color = 'red', label = 'Predicted Quantity')\nplt.xlabel('Date')\nplt.ylabel('Quantity')\nplt.legend()\nplt.title('Bombay Aloo Sales')\n","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:03:11.410000Z","iopub.execute_input":"2022-05-05T13:03:11.410697Z","iopub.status.idle":"2022-05-05T13:03:12.088789Z","shell.execute_reply.started":"2022-05-05T13:03:11.410659Z","shell.execute_reply":"2022-05-05T13:03:12.087915Z"},"trusted":true},"execution_count":168,"outputs":[]},{"cell_type":"code","source":"#Lastly metrics mean_absolute_error, r2_score, mean_squared_log_error\n#df\nprint(\"Metrics for Total Sale\\n\")\nprint(\"Mean Absolute Error:\\n\", mean_absolute_error(ytest, ypred))\nprint(\"R Squared:\\n\", r2_score(ytest, ypred))\nprint(\"Mean Squared Log Error:\\n\", mean_squared_log_error(ytest, ypred))\n\n#df2\nprint(\"\\n\")\nprint(\"Metrics for Bombay Aloo Sales\\n\")\nprint(\"Mean Absolute Error:\\n\", mean_absolute_error(ytest2, ypred2))\nprint(\"R Squared:\\n\", r2_score(ytest2, ypred2))\nprint(\"Mean Squared Log Error:\\n\", mean_squared_log_error(ytest2, ypred2))","metadata":{"execution":{"iopub.status.busy":"2022-05-05T13:05:48.988790Z","iopub.execute_input":"2022-05-05T13:05:48.989135Z","iopub.status.idle":"2022-05-05T13:05:49.006965Z","shell.execute_reply.started":"2022-05-05T13:05:48.989100Z","shell.execute_reply":"2022-05-05T13:05:49.005591Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"markdown","source":"**4. Conclusion**\n\n","metadata":{}},{"cell_type":"markdown","source":"The clearest metric is the RMSE. This shows clearly that the model for Total sales worked really quite well. The Bombay Aloo model however was less successful, most likely due to the far smaller quantity per week.\n\nSo the Total sales model looks like it would be good to start deploying, but perhaps the Bombay Aloo model should be rethought/scrapped, or alternatively changed to a monthly model, which may improve accuracy (though may not be as useful to the business)\n\nAs always, improvement could be had with cross validation, more data, hyperparameter optimisation, and possibly tryign some of the other models mentioned above)","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}