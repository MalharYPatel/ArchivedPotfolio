{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-18T08:28:07.903533Z","iopub.execute_input":"2022-04-18T08:28:07.904060Z","iopub.status.idle":"2022-04-18T08:28:07.941144Z","shell.execute_reply.started":"2022-04-18T08:28:07.903972Z","shell.execute_reply":"2022-04-18T08:28:07.940541Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings; \nwarnings.simplefilter('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:07.942586Z","iopub.execute_input":"2022-04-18T08:28:07.943038Z","iopub.status.idle":"2022-04-18T08:28:07.946954Z","shell.execute_reply.started":"2022-04-18T08:28:07.943003Z","shell.execute_reply":"2022-04-18T08:28:07.946028Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Price Predictions for Bitcoin**\n\n**1.Intro**\n\n\nIn this simple Project, we will be looking at the FB Prophet algo for time series analysis. Other options would include ARIMA, SARIMAX, as well as regression analysis using algorithms not strictly designed for time series data, such XGBoost, and Random Forest. Past machine learning and onto deep learning, LSTM is an option but it has a very long train time, and depending on the use case, the expected accuracy increase may be negligible, or not even realised. One major advantage of these ML and DL algorithms is that they can take multiple predictor variables much more easily than fbprophet. As we will see later, this is still possible in Prophet too\n\n\nThe reason I like Prophet, is that it requires minimal pre-processing of the data that the statsmodel algos tend to need for best results eg converting to logs, identifying potential seasonality, splitting stationary and non-stationary parts of the data. This makes is more useable ‘out of the box’. I also appreciate the sklearn style syntax, and that it infers potential patterns without you needing to feature engineer them. So...here we go\n\n\nFirst we import libraries and read in the data\n","metadata":{}},{"cell_type":"code","source":"!pip install pystan\n!pip install fbprophet\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom fbprophet import Prophet\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\ndata = pd.read_csv(\"../input/bitcoin-price-usd/main.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:07.948485Z","iopub.execute_input":"2022-04-18T08:28:07.949035Z","iopub.status.idle":"2022-04-18T08:28:31.696393Z","shell.execute_reply.started":"2022-04-18T08:28:07.948991Z","shell.execute_reply":"2022-04-18T08:28:31.695649Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**2. Data Wrangling and Prep**\nProhpet, in default mode, always only takes a dataframe with 2 columns, the datetime, and the predictor variable, which in this case we will take as the closing price as it is the most commonly used in financial markets. They must be labelled 'ds' and 'y'\nWe will then convert the time code from UNIX time code to something clearer, and although it does not apply to Prophet, I will inlcude a small section on how we could then engineer some basic features which might aid in prediction with XGBoost etc, including some simple moving averages, range measurements, and seasonality columns. As I said, these do not apply to the more basic Prophet model, but are there for completeness, in case you wished to run another model for comparison, or to improve the Prophet model, which we will get to later on","metadata":{}},{"cell_type":"code","source":"#Lets slice the dataframe to the 2 relevant columns, and see the data\ndf = data[['Open Time','Close']]\ndf.head(10)\ndf.dtypes","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:31.698031Z","iopub.execute_input":"2022-04-18T08:28:31.698236Z","iopub.status.idle":"2022-04-18T08:28:31.716453Z","shell.execute_reply.started":"2022-04-18T08:28:31.698211Z","shell.execute_reply":"2022-04-18T08:28:31.715808Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Convert UNIX time to pandas datetime object\ndf = df.rename({'Open Time': 'ds', 'Close': 'y'}, axis = 1)\ndf['ds']= pd.to_datetime(df['ds'],unit='ms')\ndf['ds'].head(5)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:31.719783Z","iopub.execute_input":"2022-04-18T08:28:31.720020Z","iopub.status.idle":"2022-04-18T08:28:31.739030Z","shell.execute_reply.started":"2022-04-18T08:28:31.719991Z","shell.execute_reply":"2022-04-18T08:28:31.738264Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"'''\n#N/A in this simple case, but for other ML algos like XGBoost, we would feature engineer, for eg, as below:\n\n#Add features for hour, day, month\ndf['Hour'] = df['DateTime'].dt.hour\ndf['Day'] = df['DateTime'].dt.dayofweek\ndf['Month'] = df['DateTime'].dt.month\n\n#Add 9,21,50, 100 and 200 MAs\nfor n in [9,21,50, 100, 200]:\n    name = \"MA\" + str(n)\n    df[name] = df['Close'].rolling(n).mean()\n\n#Add 5,10,20,50 period Ranges\nfor n in [5,10,20,50]:\n    name = \"Range\" + str(n)\n    df[name] = df['High'].rolling(n).max() - df['Low'].rolling(n).min()\n\n'''","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:31.740091Z","iopub.execute_input":"2022-04-18T08:28:31.740314Z","iopub.status.idle":"2022-04-18T08:28:31.746317Z","shell.execute_reply.started":"2022-04-18T08:28:31.740288Z","shell.execute_reply":"2022-04-18T08:28:31.745543Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"**3. We now create our model**\n\nThis involves splitting into test and train (which cannot be done with sklearn's train_test_split as it will shuffle the data, which is useless for a time series). We then feed data into the regressor and train it","metadata":{}},{"cell_type":"code","source":"#Split test and train data by finding the df length and doing a 0.95:0.05 split [typically 75% is more normal,\n#but I have made it smaller simply to avoid excessive script run times]\nprint(\"DataFrame Length \\n\", df.shape[0])\nprint(\"Number of rows for test \\n\", df.shape[0] * 0.95)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:31.747863Z","iopub.execute_input":"2022-04-18T08:28:31.748169Z","iopub.status.idle":"2022-04-18T08:28:31.761431Z","shell.execute_reply.started":"2022-04-18T08:28:31.748129Z","shell.execute_reply":"2022-04-18T08:28:31.760251Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train =df.iloc[:178902,:]\nytest = df.iloc[178902:]\n#for just y values, you would use df['y'].iloc[178902:], but it will be clear further along why this wasn't done","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:31.764921Z","iopub.execute_input":"2022-04-18T08:28:31.765188Z","iopub.status.idle":"2022-04-18T08:28:31.771394Z","shell.execute_reply.started":"2022-04-18T08:28:31.765150Z","shell.execute_reply":"2022-04-18T08:28:31.770807Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"**4. We now use our model to do some prediction**\n\nIn fbprophet this is done through generation of a dataframe with upper and lower predicted limits, and a predicted midline value called yhat. We can also obtain a breakdown of the effect of the explanatory factors on the outcome","metadata":{}},{"cell_type":"code","source":"model = Prophet(daily_seasonality=True)\nmodel.fit(train)\nforecast = model.make_future_dataframe(periods=9000)\n#the parameter include_history=False would normally be used for testing only, but I have included history to allow us to visualise better\nforecast = model.predict(forecast)\nforecast.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:28:31.772639Z","iopub.execute_input":"2022-04-18T08:28:31.773363Z","iopub.status.idle":"2022-04-18T08:46:43.476399Z","shell.execute_reply.started":"2022-04-18T08:28:31.773328Z","shell.execute_reply":"2022-04-18T08:46:43.475450Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Plot the Forecast\n#plot1 = model.plot(forecast)\n#This isn't very clear, lets do the first 100 results\nplt.figure(figsize = (15,8))\nplt.plot(forecast.ds.head(100), forecast.yhat.head(100))\nplt.show\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:43.478095Z","iopub.execute_input":"2022-04-18T08:46:43.478316Z","iopub.status.idle":"2022-04-18T08:46:43.773427Z","shell.execute_reply.started":"2022-04-18T08:46:43.478291Z","shell.execute_reply":"2022-04-18T08:46:43.772384Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#Prophet's built-in analysis of components\nplot2 = model.plot_components(forecast)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:43.774900Z","iopub.execute_input":"2022-04-18T08:46:43.775807Z","iopub.status.idle":"2022-04-18T08:46:46.277498Z","shell.execute_reply.started":"2022-04-18T08:46:43.775764Z","shell.execute_reply":"2022-04-18T08:46:46.276929Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"**5. Evaluation**\n\nUsing this simple 'Out of the Box' method for Prophet, lets evaluate the predicted yhat values against the actual ones from our dataset. The metrics we need to use will be for evaluating regression problems. There are many to chose from, but I have used some of the most common ones","metadata":{}},{"cell_type":"code","source":"#We have to use sklearn for evaluation metrics as none come built in with Prophet\n#for the test part of the dataframe, we do not know for sure if there are missing dates which might throw the prediction \n#and actual series out of synch. A good approach is to create a joined table and clear nan values, to ensure the data\n# is the same length, and properly synched\n\nright_df = forecast[['ds','yhat']] \nnew_df = pd.merge(ytest, right_df, how = 'inner', on = 'ds')\n\nmse = mean_squared_error(new_df['y'],new_df['yhat'])\nprint(\"Mean Squared Error:\", mse)\n\nmae = mean_absolute_error(new_df['y'],new_df['yhat'])\nprint(\"Mean Absolute Error:\", mae)\n\nr2 = r2_score(new_df['y'],new_df['yhat'])\nprint(\"R Squared Score:\", r2)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:46.278581Z","iopub.execute_input":"2022-04-18T08:46:46.278913Z","iopub.status.idle":"2022-04-18T08:46:46.337440Z","shell.execute_reply.started":"2022-04-18T08:46:46.278885Z","shell.execute_reply":"2022-04-18T08:46:46.336823Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**6. Follow-Up**\n\nAs is obvious from above, the model is pretty terrible. Its not really a surprise. Bitcoin is a very volatile and unpredictable asset, and the parameters in Prophet alone are not sufficient to predict its future state. One ideally needs a model which is more able to incorporate additional factors into the regression analysis as predictor variables.\n\nProphet can actually do this, by way of a function called 'Adding Regressors'. I will demonstrate this below (if it is not compiled on the notebook, it is only because it was taking too long on my PC!!)\n\nOne final note, is that where there are \nand see if this can improve our model","metadata":{}},{"cell_type":"code","source":"#First we trim our Dataframe to the relevant columns, and rename as needed\ndata = data[['Open Time','Open', 'High', 'Low', 'Close', 'Volume']]\ndata = data.rename({'Open Time': 'ds', 'Close': 'y'}, axis = 1)\ndata['ds']= pd.to_datetime(data['ds'],unit='ms')","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:46.338791Z","iopub.execute_input":"2022-04-18T08:46:46.339218Z","iopub.status.idle":"2022-04-18T08:46:46.355459Z","shell.execute_reply.started":"2022-04-18T08:46:46.339187Z","shell.execute_reply":"2022-04-18T08:46:46.354422Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now we add the needed features\n\n#Add 9,21,50, 100 and 200 MAs\nfor n in [9,21,50, 100, 200]:\n    name = \"MA\" + str(n)\n    data[name] = data['y'].rolling(n).mean()\n\n#Add 5,10,20,50 period Ranges\nfor n in [5,10,20,50]:\n    name = \"Range\" + str(n)\n    data[name] = data['High'].rolling(n).max() - data['Low'].rolling(n).min()\n\n#Remove nan cloums created from the above (where there were insufficient observations to make the calculations)\ndata = data.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:46.356876Z","iopub.execute_input":"2022-04-18T08:46:46.357119Z","iopub.status.idle":"2022-04-18T08:46:46.482681Z","shell.execute_reply.started":"2022-04-18T08:46:46.357090Z","shell.execute_reply":"2022-04-18T08:46:46.481930Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"data = data.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:46.484124Z","iopub.execute_input":"2022-04-18T08:46:46.484947Z","iopub.status.idle":"2022-04-18T08:46:46.508214Z","shell.execute_reply.started":"2022-04-18T08:46:46.484915Z","shell.execute_reply":"2022-04-18T08:46:46.507324Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#next we divide it into train and test\ntrain_X= data[:178902]\ntest_X= data[178902:]","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:46.509262Z","iopub.execute_input":"2022-04-18T08:46:46.509477Z","iopub.status.idle":"2022-04-18T08:46:46.514209Z","shell.execute_reply.started":"2022-04-18T08:46:46.509450Z","shell.execute_reply":"2022-04-18T08:46:46.513210Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#Now we add the Regressors\nnew_model= Prophet()\nregressors = ['Open', 'High', 'Low', 'Volume', 'MA9', 'MA21', 'MA50', 'MA100', 'MA200', 'Range5', 'Range10', 'Range20', 'Range50']\nfor r in regressors:\n    new_model.add_regressor(r)\n\n#Fit the data to the updated model\nnew_model.fit(train_X)\nfuture_data = new_model.make_future_dataframe(periods=1000)\n\n#forecast the prices for the Test  data (a direct method vs the indirect method described above)\nforecast_data = new_model.predict(test_X)\nnew_model.plot(forecast_data)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T08:46:46.515891Z","iopub.execute_input":"2022-04-18T08:46:46.516918Z","iopub.status.idle":"2022-04-18T09:08:18.172067Z","shell.execute_reply.started":"2022-04-18T08:46:46.516873Z","shell.execute_reply":"2022-04-18T09:08:18.170713Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#Evaluate the new model\nright_df = forecast_data[['ds','yhat']] \nnew_df = pd.merge(test_X[['ds','y']], right_df, how = 'inner', on = 'ds')\n\nmse = mean_squared_error(new_df['y'],new_df['yhat'])\nprint(\"Mean Squared Error:\", mse)\n\nmae = mean_absolute_error(new_df['y'],new_df['yhat'])\nprint(\"Mean Absolute Error:\", mae)\n\nr2 = r2_score(new_df['y'],new_df['yhat'])\nprint(\"R Squared Score:\", r2)","metadata":{"execution":{"iopub.status.busy":"2022-04-18T09:08:18.174109Z","iopub.execute_input":"2022-04-18T09:08:18.174350Z","iopub.status.idle":"2022-04-18T09:08:18.194730Z","shell.execute_reply.started":"2022-04-18T09:08:18.174323Z","shell.execute_reply":"2022-04-18T09:08:18.193846Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Result! By adding regressors, we have improved our model considerably. It could still use more work, via hyperparameter cross validation, but we can clearly see here, that we need more than just a time series of data to make predictions with prophet. As a general algorithm, it failed to take into account the most common metrics used with financial data. This is where a data scientist with domain knowledge pays dividends over an 'out of the box' model.","metadata":{}}]}