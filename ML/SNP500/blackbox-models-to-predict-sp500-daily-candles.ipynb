{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-28T11:22:16.762320Z","iopub.execute_input":"2022-03-28T11:22:16.763062Z","iopub.status.idle":"2022-03-28T11:22:16.776201Z","shell.execute_reply.started":"2022-03-28T11:22:16.763025Z","shell.execute_reply":"2022-03-28T11:22:16.775185Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier # Stochastic Gradient descent using a range of linear classifiers\nfrom sklearn.svm import LinearSVC #Support Vector Classifier with linear kernal to scale better to larger sampel sizes\nfrom sklearn.naive_bayes import GaussianNB #Gaussian Naive Bayes classifier\nfrom sklearn.neighbors import KNeighborsClassifier #Classifier implementing the k-nearest neighbors vote\n\nfrom sklearn.metrics import precision_score, accuracy_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\ndf = pd.read_csv(\"../input/snp500-max/GSPC latest snp.csv\")\nprint(df.head(10))\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2022-03-28T11:22:16.798788Z","iopub.execute_input":"2022-03-28T11:22:16.799274Z","iopub.status.idle":"2022-03-28T11:22:16.853471Z","shell.execute_reply.started":"2022-03-28T11:22:16.799237Z","shell.execute_reply":"2022-03-28T11:22:16.852641Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#a) Some cleaning to start with\ndf= df[df['Volume'] != 0]\ndf = df.drop('Adj Close', axis = 1)\n#Now Some Feature Engineering \n#b)Dates\ndf['Date'] = pd.to_datetime(df['Date'])\ndf['Day'] = df['Date'].dt.day_name()\ndf['Month'] = df['Date'].dt.month_name()\ndf = df.drop('Date', axis = 1)\n#c)Difference between close and \n#Moving Averages, Bollinger Bands (at 2 stdev), ranges and Donchian Channels(Highest High and Lowest Low)\nfor n in range(5,205,5):\n    MA_name = \"MA\" + str(n)\n    Range_name = \"Range\" + str(n)\n    UpperBB_name = \"UpperBB\" + str(n)\n    LowerBB_name = \"LowerBB\" + str(n)\n    HH_name = \"HH\" + str(n)\n    LL_name = \"LL\" + str(n)\n    df[MA_name] = df['Close'].rolling(n).mean() - df['Close']\n    df[UpperBB_name] = df[MA_name] + 2 * (df[MA_name].rolling(n).std())- df['Close']\n    df[LowerBB_name] = df[MA_name] - 2 * (df[MA_name].rolling(n).std())- df['Close']\n    df[Range_name] = df['High'].rolling(n).max() - df['Low'].rolling(n).min()- df['Close']\n    df[HH_name] = df['High'].rolling(n).max()- df['Close']\n    df[HH_name] = df['Low'].rolling(n).min()- df['Close']","metadata":{"execution":{"iopub.status.busy":"2022-03-28T11:22:16.855663Z","iopub.execute_input":"2022-03-28T11:22:16.855946Z","iopub.status.idle":"2022-03-28T11:22:17.273691Z","shell.execute_reply.started":"2022-03-28T11:22:16.855907Z","shell.execute_reply":"2022-03-28T11:22:17.272682Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#One Hot Encode ordinal catagorical variables\ncategorical_columns = ['Day', 'Month']\nfor column in categorical_columns:\n    tempdf = pd.get_dummies(df[column], prefix=column)\n    df = pd.merge(\n        left=df,\n        right=tempdf,\n        left_index=True,\n        right_index=True,\n    )\n    df = df.drop(columns=column)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T11:22:17.275702Z","iopub.execute_input":"2022-03-28T11:22:17.276093Z","iopub.status.idle":"2022-03-28T11:22:17.367105Z","shell.execute_reply.started":"2022-03-28T11:22:17.276049Z","shell.execute_reply":"2022-03-28T11:22:17.366169Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Drop Nan values, Create dependent variable, ie whether next bar is positive/Bull or negative/bear (1 =+ve, 0 = negative)\ndf= df.dropna()\ndf['Y'] = (df['Close'].shift(-1) - df['Open'].shift(-1)) > 0\n#defragment\ntempdf = df.copy()\ndf = tempdf\nX = df.drop('Y', axis = 1)\ny = df['Y']","metadata":{"execution":{"iopub.status.busy":"2022-03-28T11:22:17.368373Z","iopub.execute_input":"2022-03-28T11:22:17.368714Z","iopub.status.idle":"2022-03-28T11:22:17.416271Z","shell.execute_reply.started":"2022-03-28T11:22:17.368681Z","shell.execute_reply":"2022-03-28T11:22:17.415260Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"#Split test and train data\nxtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size = 0.25, random_state = 0)\n#Scale dependant variables\nsc = StandardScaler()\nxtrain = sc.fit_transform(xtrain)\nxtest = sc.transform(xtest)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T11:22:17.418031Z","iopub.execute_input":"2022-03-28T11:22:17.418368Z","iopub.status.idle":"2022-03-28T11:22:17.513410Z","shell.execute_reply.started":"2022-03-28T11:22:17.418329Z","shell.execute_reply":"2022-03-28T11:22:17.512357Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Fit all models\nRF = RandomForestClassifier()\nSGD = SGDClassifier()\nSVM = LinearSVC(max_iter = 100000)\nNB = GaussianNB()\nKNN = KNeighborsClassifier()\n\nRF.fit(xtrain, ytrain)\nSGD.fit(xtrain, ytrain)\nSVM.fit(xtrain, ytrain)\nNB.fit(xtrain, ytrain)\nKNN.fit(xtrain, ytrain)","metadata":{"execution":{"iopub.status.busy":"2022-03-28T11:22:17.514408Z","iopub.execute_input":"2022-03-28T11:22:17.514929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Test the models, and return metrics for each model\nAcc = []\nprec = []\nmodels = ['Random Forest', 'Stochastic Gradient Descent', 'Support Vector Machine', 'Naive Bayes', 'K-Nearest Neighbor']\ny_pred = RF.predict(xtest)\nAcc.append = accuracy_score(ytest, y_pred)\nprec.append = precision_score(ytest, y_pred)\n\ny_pred = SGD.predict(xtest)\nAcc.append = accuracy_score(ytest, y_pred)\nprec.append = precision_score(ytest, y_pred)\n\ny_pred = SVM.predict(xtest)\nAcc.append = accuracy_score(ytest, y_pred)\nprec.append = precision_score(ytest, y_pred)\n\ny_pred = NB.predict(xtest)\nAcc.append = accuracy_score(ytest, y_pred)\nprec.append = precision_score(ytest, y_pred)\n\ny_pred = KNN.predict(xtest)\nAcc.append = accuracy_score(ytest, y_pred)\nprec.append = precision_score(ytest, y_pred)\n\noutput = pd.DataFrame([models, Acc, prec], columns = ['Model', 'Accuracy', 'Precision'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}